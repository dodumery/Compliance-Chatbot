
import { GoogleGenAI, GenerateContentResponse } from "@google/genai";
import { AuditStatus, AuditReport, RegulationSource } from "../types";

export class GeminiService {
  private ai: GoogleGenAI;

  constructor() {
    this.ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
  }

  async runAudit(sources: RegulationSource[], scenario: string, useSearch: boolean): Promise<AuditReport> {
    const regulationText = sources.map(s => `[SOURCE: ${s.name}]\n${s.content}`).join('\n---\n');
    
    const visualParts = sources.flatMap(s => s.visualContext || []).map(base64 => ({
      inlineData: {
        data: base64.split(',')[1] || base64,
        mimeType: 'image/png'
      }
    }));

    const promptText = `
      You are a high-precision Compliance Auditor. 
      Analyze the [Scenario] against the [Regulation Text] and [Visual Layout Images].
      
      [Scenario]:
      ${scenario}

      [Regulation Text]:
      ${regulationText}
      
      CRITICAL INSTRUCTIONS (SPEED & STRUCTURE):
      1. BE CONCISE: Provide your analysis within 10 seconds. Focus on the core mapping.
      2. TABLE USAGE: ALWAYS summarize the mapping between scenario and clauses in a Markdown Table for clarity.
      3. HIGHLIGHTING: Use <span class="highlight-red">text</span> for violation triggers.
      4. BREADCRUMB STYLE: When referencing clauses, follow a hierarchical path structure.
         Example: [ì¹´í…Œê³ ë¦¬] > [í•˜ìœ„ í•­ëª©] > [ì„¸ë¶€ ì¡°í•­]
         Specifically for 'Approval Authority Regulations (ì „ê²°ê·œì •)', follow this style:
         'ì½˜í…ì¸  > 1. ì½˜í…ì¸  ê³„ì•½ > ì‹ ê·œê³„ì•½'
      
      Output Format:
      ### âš–ï¸ íŒì • ê²°ê³¼: [ìœ„ë°˜ / ì í•© / íŒë‹¨ ë¶ˆê°€]
      ### ğŸ“œ ê´€ë ¨ ê·¼ê±° ì¡°í•­
      > (Hierarchical Path Example: ì½˜í…ì¸  > 1.ì½˜í…ì¸  ê³„ì•½ > ì‹ ê·œê³„ì•½)
      > (ê·œì • ì›ë¬¸ ì¡°í•­ ì¸ìš©)
      ### ğŸ” ìƒì„¸ ë¶„ì„
      (ì‚¬ì•ˆ-ì¡°í•­ ë§¤í•‘ í…Œì´ë¸” í¬í•¨)
      ### ğŸ’¡ ì¡°ì¹˜ ê¶Œê³  ì‚¬í•­
      - (í•µì‹¬ ì¡°ì¹˜ ì‚¬í•­)
    `;

    const response = await this.ai.models.generateContent({
      model: 'gemini-3-pro-preview',
      contents: {
        parts: [
          ...visualParts,
          { text: promptText }
        ]
      },
      config: {
        tools: useSearch ? [{ googleSearch: {} }] : undefined,
        thinkingConfig: { thinkingBudget: 1024 }
      },
    });

    const text = response.text || "No response generated.";
    let status = AuditStatus.UNCERTAIN;
    if (text.includes('ìœ„ë°˜')) status = AuditStatus.VIOLATION;
    else if (text.includes('ì í•©')) status = AuditStatus.COMPLIANT;

    const groundingChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks;
    const urls = groundingChunks?.map((chunk: any) => ({
      uri: chunk.web?.uri || chunk.maps?.uri,
      title: chunk.web?.title || chunk.maps?.title || "Reference"
    })).filter((u: any) => u.uri);

    return { status, rawMarkdown: text, groundingUrls: urls };
  }

  async askQuestion(sources: RegulationSource[], question: string): Promise<string> {
    const regulationText = sources.map(s => `[SOURCE: ${s.name}]\n${s.content}`).join('\n---\n');
    const visualParts = sources.flatMap(s => s.visualContext || []).map(base64 => ({
      inlineData: {
        data: base64.split(',')[1] || base64,
        mimeType: 'image/png'
      }
    }));

    const promptText = `
      You are a helpful Regulation Expert. Explain the [Question] based strictly on the [Regulation].
      
      [Question]: ${question}
      [Regulation Text]: ${regulationText}
      
      INSTRUCTIONS (SPEED & STRUCTURE):
      - If the source has a table, RECREATE it as a Markdown Table.
      - Be direct and professional. Target <10s response time.
      - BREADCRUMB STYLE: Use hierarchical path for references (e.g., 'ì½˜í…ì¸  > 1.ì½˜í…ì¸  ê³„ì•½ > ì‹ ê·œê³„ì•½').
      
      Output Format:
      ### â„¹ï¸ ì§ˆë¬¸ í•´ì„¤: [ìš”ì•½]
      ### ğŸ“– ìƒì„¸ ê·¼ê±° ë° í…Œì´ë¸” í•´ì„¤
      (í…Œì´ë¸” í¬í•¨ ìƒì„¸ ì„¤ëª…)
    `;

    const response = await this.ai.models.generateContent({
      model: 'gemini-3-pro-preview',
      contents: {
        parts: [...visualParts, { text: promptText }]
      },
      config: { 
        thinkingConfig: { thinkingBudget: 1024 } 
      }
    });

    return response.text || "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.";
  }

  async editImage(base64Image: string, prompt: string): Promise<string> {
    const cleanBase64 = base64Image.split(',')[1] || base64Image;
    const response = await this.ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: [
          { inlineData: { data: cleanBase64, mimeType: 'image/png' } },
          { text: prompt },
        ],
      },
    });
    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) return `data:image/png;base64,${part.inlineData.data}`;
    }
    throw new Error("No image generated");
  }
}
